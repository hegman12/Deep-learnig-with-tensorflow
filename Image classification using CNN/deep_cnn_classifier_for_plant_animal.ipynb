{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=np.load(\"C:\\\\MLDatabases\\\\google-images-download-master\\\\downloads\\\\100x100_resized_images_as_numpy\\\\x_non_alpha_composed.npy\",mmap_mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels=np.load(\"C:\\\\MLDatabases\\\\google-images-download-master\\\\downloads\\\\100x100_resized_images_as_numpy\\\\y_non_alpha_composite1.npy\",mmap_mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_weights(shape,name,stddev=0.003):\n",
    "    return tf.Variable(initial_value=tf.truncated_normal(shape=shape, mean=0, stddev=stddev, dtype=\"float32\", seed=10),dtype=\"float32\",name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.set_random_seed(10)\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "def get_graph():\n",
    "    \n",
    "    strides=(1,1,1,1)    \n",
    "    \n",
    "    with tf.device(\"/device:CPU:0\"):\n",
    "    \n",
    "        x=tf.placeholder(tf.float32,shape=(None,100,100,3),name=\"x\")\n",
    "        y=tf.placeholder(tf.int64,shape=(None,),name=\"y\")\n",
    "        lr=tf.placeholder(tf.float32,shape=(),name=\"lr\")\n",
    "        dr=tf.placeholder(tf.float32,shape=(),name=\"dr\")\n",
    "        #y_onehot=tf.one_hot(y,2)\n",
    "\n",
    "\n",
    "        conv_w1=create_weights((3,3,3,2),\"conv_w1\")\n",
    "        conv_w2=create_weights((3,3,2,2),\"conv_w2\")\n",
    "        conv_w3=create_weights((3,3,4,2),\"conv_w3\")\n",
    "        conv_w4=create_weights((3,3,8,2),\"conv_w4\")\n",
    "        conv_w4=create_weights((3,3,8,4),\"conv_w5\")\n",
    "        conv_w4=create_weights((3,3,8,4),\"conv_w6\")\n",
    "        conv_w4=create_weights((3,3,8,4),\"conv_w7\")\n",
    "        conv_w4=create_weights((3,3,8,4),\"conv_w8\")\n",
    "        conv_w4=create_weights((3,3,8,4),\"conv_w9\")\n",
    "        conv_w4=create_weights((3,3,8,8),\"conv_w10\")\n",
    "        conv_w4=create_weights((3,3,8,8),\"conv_w11\")\n",
    "        conv_w4=create_weights((3,3,8,8),\"conv_w12\")\n",
    "        conv_w4=create_weights((3,3,8,8),\"conv_w13\")\n",
    "        conv_w4=create_weights((3,3,8,8),\"conv_w14\")\n",
    "        #conv1_mean=create_weights((16,),\"conv1_mean\",1)\n",
    "        #conv2_mean=create_weights((16,),\"conv2_mean\",1)\n",
    "        #conv3_mean=create_weights((16,),\"conv3_mean\",1)\n",
    "        #conv4_mean=create_weights((16,),\"conv4_mean\",1)\n",
    "        #conv1_variance=create_weights((16,),\"conv1_variance\",1)\n",
    "        #conv2_variance=create_weights((16,),\"conv2_variance\",1)\n",
    "        #conv3_variance=create_weights((16,),\"conv3_variance\",1)\n",
    "        #conv4_variance=create_weights((16,),\"conv4_variance\",1)\n",
    "\n",
    "        w_fc1=create_weights((7*7*8,512),\"w_fc1\")\n",
    "        fc1_bias=create_weights((512,),\"fc1_bias\")\n",
    "        w_fc2=create_weights((512,2),\"w_fc2\")\n",
    "        fc2_bias=create_weights((2,),\"fc2_bias\")\n",
    "\n",
    "\n",
    "\n",
    "        conv1=tf.nn.conv2d(input=x,filter=conv_w1,strides=strides,padding=\"SAME\",name=\"conv1\") #conv1=45\n",
    "        conv1_relu=tf.nn.relu(conv1,name=\"relu_conv1\")\n",
    "        conv1_bn=tf.layers.batch_normalization(conv1_relu,training=True)\n",
    "        #conv1_bn=tf.nn.batch_normalization(x=conv1_relu,mean=conv1_mean,variance=conv1_variance,variance_epsilon=0.000004,offset=None,scale=None)\n",
    "\n",
    "\n",
    "        conv2=tf.nn.conv2d(input=conv1_bn,filter=conv_w2,strides=strides,padding=\"SAME\",name=\"conv2\") #conv2=21\n",
    "        conv2_relu=tf.nn.relu(conv2,name=\"relu_conv2\")\n",
    "        conv2_bn=tf.layers.batch_normalization(conv2_relu,training=True)\n",
    "        #conv2_bn=tf.nn.batch_normalization(x=conv2_relu,mean=conv2_mean,variance=conv2_variance,variance_epsilon=0.000004,offset=None,scale=None)\n",
    "\n",
    "\n",
    "        conv3=tf.nn.conv2d(input=conv2_bn,filter=conv_w3,strides=strides,padding=\"SAME\",name=\"conv3\") #conv3=10\n",
    "        conv3_relu=tf.nn.relu(conv3,name=\"relu_conv3\")\n",
    "        conv3_bn=tf.layers.batch_normalization(conv3_relu,training=True)\n",
    "        #conv3_bn=tf.nn.batch_normalization(x=conv3_relu,mean=conv3_mean,variance=conv3_variance,variance_epsilon=0.000004,offset=None,scale=None)\n",
    "\n",
    "\n",
    "        conv4=tf.nn.conv2d(input=conv3_bn,filter=conv_w4,strides=strides,padding=\"SAME\",name=\"conv4\") #conv4=5\n",
    "        conv4_relu=tf.nn.relu(conv4,name=\"relu_conv4\")\n",
    "        conv4_bn=tf.layers.batch_normalization(conv4_relu,training=True)\n",
    "\n",
    "        #conv4_bn=tf.nn.batch_normalization(x=conv4_relu,mean=conv4_mean,variance=conv4_variance,variance_epsilon=0.000004,offset=None,scale=None)\n",
    "        flat_feature=tf.reshape(tensor=conv4_bn,shape=[-1,7*7*8],name=\"flat_feature\")\n",
    "        print(flat_feature)\n",
    "        fc1=tf.add(tf.matmul(flat_feature,w_fc1),fc1_bias,name=\"fc1\")\n",
    "        fc1_dropout=tf.nn.dropout(fc1,keep_prob=dr)\n",
    "\n",
    "        logits=tf.add(tf.matmul(fc1_dropout,w_fc2),fc2_bias,name=\"fc2\")\n",
    "\n",
    "        with tf.device(\"/device:CPU:0\"):\n",
    "            softmaxed_logits=tf.nn.softmax(logits,name=\"softmaxed_logits\")    \n",
    "        loss=tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(None,labels=y, logits=logits),name=\"loss\")\n",
    "\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            optimizer=tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
    "\n",
    "        accuracy=tf.reduce_mean(tf.cast(tf.equal(tf.argmax(softmaxed_logits,axis=1), y),dtype=tf.float32))\n",
    "    \n",
    "    return optimizer,loss,accuracy,x,y,lr,dr\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
