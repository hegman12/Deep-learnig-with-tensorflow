{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import collections\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary_size = 500000\n",
    "data_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=np.load('gutenburg_as_ids_500000.npy',mmap_mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "  global data_index\n",
    "  assert batch_size % num_skips == 0\n",
    "  assert num_skips <= 2 * skip_window\n",
    "  batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "  labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "  span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n",
    "  buffer = collections.deque(maxlen=span)  # pylint: disable=redefined-builtin\n",
    "  if data_index + span > len(data):\n",
    "    data_index = 0\n",
    "  buffer.extend(data[data_index:data_index + span])\n",
    "  data_index += span\n",
    "  for i in range(batch_size // num_skips):\n",
    "    context_words = [w for w in range(span) if w != skip_window]\n",
    "    words_to_use = random.sample(context_words, num_skips)\n",
    "    for j, context_word in enumerate(words_to_use):\n",
    "      batch[i * num_skips + j] = buffer[skip_window]\n",
    "      labels[i * num_skips + j, 0] = buffer[context_word]\n",
    "    if data_index == len(data):\n",
    "      buffer.extend(data[0:span])\n",
    "      data_index = span\n",
    "    else:\n",
    "      buffer.append(data[data_index])\n",
    "      data_index += 1\n",
    "  # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "  data_index = (data_index + len(data) - span) % len(data)\n",
    "  return batch, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "embedding_size = 100  # Dimension of the embedding vector.\n",
    "skip_window = 1  # How many words to consider left and right.\n",
    "num_skips = 2  # How many times to reuse an input to generate a label.\n",
    "num_sampled = 64  # Number of negative examples to sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(10)\n",
    "def graph():\n",
    "\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        x=tf.placeholder(tf.int32,shape=(batch_size),name=\"x\")\n",
    "        y=tf.placeholder(tf.int32,shape=(batch_size,1),name=\"y\")\n",
    "        embedding_matrix=tf.Variable(tf.random_uniform(shape=(vocabulary_size,embedding_size),maxval=1.0/embedding_size,minval=-1.0/embedding_size),name=\"embedding_matrix\")\n",
    "    \n",
    "        softmax_weight=tf.Variable(tf.truncated_normal(shape=(vocabulary_size,embedding_size),mean=0,stddev=0.003),name=\"softmax_weight\")\n",
    "        softmax_bias=tf.Variable(tf.zeros(shape=(vocabulary_size,)),name=\"softmax_bias\")\n",
    "\n",
    "        embed=tf.nn.embedding_lookup(embedding_matrix,x)\n",
    "        \n",
    "        with tf.device('/device:CPU:0'):\n",
    "            loss=tf.reduce_mean(tf.nn.nce_loss(weights=softmax_weight,biases=softmax_bias,labels=y,inputs=embed,num_sampled=num_sampled,num_classes=vocabulary_size,remove_accidental_hits=True,partition_strategy=\"div\"))\n",
    "        train_op=tf.train.GradientDescentOptimizer(0.01).minimize(loss)\n",
    "\n",
    "        normalized_embedding=tf.nn.l2_normalize(embedding_matrix)\n",
    "        \n",
    "    return x,y,embedding_matrix,normalized_embedding,loss,train_op\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x,y,embedding_matrix,normalized_embedding,loss,train_op=graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init=tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver=tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 Loss:  364.606\n",
      "Iteration 50000 Loss:  178.509\n",
      "Iteration 100000 Loss:  100.238\n",
      "Iteration 150000 Loss:  91.1575\n",
      "Iteration 200000 Loss:  89.6562\n",
      "Iteration 250000 Loss:  51.5277\n",
      "Iteration 300000 Loss:  116.913\n",
      "Iteration 350000 Loss:  78.2076\n",
      "Iteration 400000 Loss:  55.2738\n",
      "Iteration 450000 Loss:  67.8976\n",
      "Iteration 500000 Loss:  70.5004\n",
      "Iteration 550000 Loss:  64.7648\n",
      "Iteration 600000 Loss:  50.826\n",
      "Iteration 650000 Loss:  62.6653\n",
      "Iteration 700000 Loss:  39.7162\n",
      "Iteration 750000 Loss:  60.6925\n",
      "Iteration 800000 Loss:  50.1791\n",
      "Iteration 850000 Loss:  32.0693\n",
      "Iteration 900000 Loss:  43.4146\n",
      "Iteration 950000 Loss:  46.0973\n",
      "Iteration 1000000 Loss:  26.8475\n",
      "Iteration 1050000 Loss:  26.7666\n",
      "Iteration 1100000 Loss:  33.9741\n",
      "Iteration 1150000 Loss:  61.5399\n",
      "Iteration 1200000 Loss:  32.8451\n",
      "Iteration 1250000 Loss:  32.8994\n",
      "Iteration 1300000 Loss:  23.8557\n",
      "Iteration 1350000 Loss:  34.9174\n",
      "Iteration 1400000 Loss:  12.5337\n",
      "Iteration 1450000 Loss:  20.6105\n",
      "Iteration 1500000 Loss:  48.4436\n",
      "Iteration 1550000 Loss:  58.8773\n",
      "Iteration 1600000 Loss:  17.7007\n",
      "Iteration 1650000 Loss:  39.6993\n",
      "Iteration 1700000 Loss:  38.9076\n",
      "Iteration 1750000 Loss:  47.2826\n",
      "Iteration 1800000 Loss:  23.2721\n",
      "Iteration 1850000 Loss:  45.1485\n",
      "Iteration 1900000 Loss:  40.743\n",
      "Iteration 1950000 Loss:  27.3422\n",
      "Iteration 2000000 Loss:  39.0225\n",
      "Iteration 2050000 Loss:  56.2582\n",
      "Iteration 2100000 Loss:  16.6812\n",
      "Iteration 2150000 Loss:  29.5483\n",
      "Iteration 2200000 Loss:  33.3657\n",
      "Iteration 2250000 Loss:  23.2788\n",
      "Iteration 2300000 Loss:  37.572\n",
      "Iteration 2350000 Loss:  16.037\n",
      "Iteration 2400000 Loss:  21.513\n",
      "Iteration 2450000 Loss:  20.6859\n",
      "Iteration 2500000 Loss:  35.5322\n",
      "Iteration 2550000 Loss:  23.5313\n",
      "Iteration 2600000 Loss:  50.1125\n",
      "Iteration 2650000 Loss:  21.1382\n",
      "Iteration 2700000 Loss:  31.7202\n",
      "Iteration 2750000 Loss:  40.1776\n",
      "Iteration 2800000 Loss:  16.4258\n",
      "Iteration 2850000 Loss:  24.83\n",
      "Iteration 2900000 Loss:  17.9476\n",
      "Iteration 2950000 Loss:  28.8949\n",
      "Iteration 3000000 Loss:  16.5108\n",
      "Iteration 3050000 Loss:  23.3379\n",
      "Iteration 3100000 Loss:  35.1615\n",
      "Iteration 3150000 Loss:  22.4071\n",
      "Iteration 3200000 Loss:  27.6393\n",
      "Iteration 3250000 Loss:  17.0781\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=tf.get_default_graph()) as sess:\n",
    "    init.run()\n",
    "    for iteration in range((len(data)//batch_size)*2):\n",
    "        batch_x,batch_y=generate_batch(batch_size,num_skips,skip_window=1)\n",
    "        \n",
    "        fd={x:batch_x,y:batch_y}        \n",
    "        l,_=sess.run([loss,train_op],fd)\n",
    "        \n",
    "        if iteration%50000==0:\n",
    "            print(\"Iteration\",iteration,\"Loss: \",l)\n",
    "    nem=normalized_embedding.eval()\n",
    "    np.save(\"normalized_embed\",nem)\n",
    "    saver.save(sess,\"C:\\\\MLDatabases\\\\JNotebooks\\\\word embedding training\\\\models\\\\skipgram\")\n",
    "del data\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-641da8fda039>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mdel\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
